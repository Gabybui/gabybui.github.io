{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q1: Explore directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /data/msd/audio/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs:///data/msd/genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs:///data/msd/main/summary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -du -s -h /data/msd/main/summary/   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -R hdfs:///data/msd/ | awk '{print $8}' | sed -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -du -s -h /data/msd/audio/\n",
    "!hdfs dfs -du -s -h /data/msd/genre/\n",
    "!hdfs dfs -du -s -h /data/msd/main/\n",
    "!hdfs dfs -du -s -h /data/msd/tasteprofile\n",
    "!hdfs dfs -du -s -h /data/msd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Count the number of rows in each of the datasets.\n",
    "!hdfs dfs -ls -R hdfs:///data/msd/ | awk '{print $8}' | while read -r i; do echo \"$i\"; hdfs dfs -cat \"$i\" | wc -l; done > linecount1.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Print function docstrings\n",
    "\n",
    "help(start_spark)\n",
    "help(stop_spark)\n",
    "help(display_spark)\n",
    "help(show_as_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook (don't actually use this many resources)\n",
    "\n",
    "start_spark(executor_instances=8, executor_cores=4, worker_memory=8, master_memory=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports and code here or insert cells below\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from pretty import SparkPretty  # download pretty.py from LEARN and put it in your M:\\ or home directory\n",
    "pretty = SparkPretty(limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine ideal number of partitions\n",
    "\n",
    "conf = sc.getConf()\n",
    "\n",
    "N = int(conf.get(\"spark.executor.instances\"))\n",
    "M = int(conf.get(\"spark.executor.cores\"))\n",
    "partitions = 4 * N * M\n",
    "\n",
    "print(f'ideal # partitions = {partitions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !hdfs dfs -cat /data/msd/tasteprofile/mismatches/sid_matches_manually_accepted.txt | head\n",
    "# !hdfs dfs -cat /data/msd/tasteprofile/mismatches/sid_matches_manually_accepted.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatches_schema = StructType([\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"song_artist\", StringType(), True),\n",
    "    StructField(\"song_title\", StringType(), True),\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "    StructField(\"track_artist\", StringType(), True),\n",
    "    StructField(\"track_title\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse mismatches in Python and then parallelize and createDataFrame in spark\n",
    "path = \"/scratch-network/courses/2023/DATA420-23S1/data/msd/tasteprofile/mismatches/sid_matches_manually_accepted.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    sid_matches_manually_accepted = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"< ERROR: \"):\n",
    "            a = line[10:28]\n",
    "            b = line[29:47]\n",
    "            c, d = line[49:-1].split(\"  !=  \")\n",
    "            e, f = c.split(\"  -  \")\n",
    "            g, h = d.split(\"  -  \")\n",
    "            sid_matches_manually_accepted.append((a, e, f, b, g, h))\n",
    "\n",
    "matches_manually_accepted = spark.createDataFrame(sc.parallelize(sid_matches_manually_accepted, 8), schema=mismatches_schema)\n",
    "show_as_html(matches_manually_accepted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/scratch-network/courses/2023/DATA420-23S1/data/msd/tasteprofile/mismatches/sid_mismatches.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    sid_mismatches = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"ERROR: \"):\n",
    "            a = line[8:26]\n",
    "            b = line[27:45]\n",
    "            c, d = line[47:-1].split(\"  !=  \")\n",
    "            e, f = c.split(\"  -  \")\n",
    "            g, h = d.split(\"  -  \")\n",
    "            sid_mismatches.append((a, e, f, b, g, h))\n",
    "\n",
    "mismatches = spark.createDataFrame(sc.parallelize(sid_mismatches, 64), schema=mismatches_schema)\n",
    "show_as_html(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse triplets in spark\n",
    "\n",
    "triplets_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"song_id\", StringType(), True),\n",
    "    StructField(\"plays\", IntegerType(), True)\n",
    "])\n",
    "triplets = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .option(\"codec\", \"gzip\")\n",
    "    .schema(triplets_schema)\n",
    "    .load(\"hdfs:///data/msd/tasteprofile/triplets.tsv/\")\n",
    "    .cache()\n",
    ")\n",
    "show_as_html(triplets,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anti join mismatches to manually accepted, and anti join the remaining mismatches to triplets\n",
    "\n",
    "mismatches_not_accepted = mismatches.join(matches_manually_accepted, on=\"song_id\", how=\"left_anti\")\n",
    "triplets_not_mismatched = triplets.join(mismatches_not_accepted, on=\"song_id\", how=\"left_anti\")\n",
    "\n",
    "print(f\"matches_manually_accepted = {matches_manually_accepted.count()}\")\n",
    "print(f\"mismatches                = {mismatches.count()}\")\n",
    "print(f\"triplets                  = {triplets.count()}\")\n",
    "print(f\"triplets_not_mismatched   = {triplets_not_mismatched.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 (b) ###\n",
    "\n",
    "- Load audio feature attribute names and types from `audio/attributes`\n",
    "- Define schemas based on `audio/attributes`\n",
    "- Load one of the small datasets from `audio/features` to use for Audio Similarity Q1 (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset names and attribute type mapping\n",
    "\n",
    "datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-methods-of-moments-all-v1.0',\n",
    "    'msd-jmir-mfcc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-jmir-spectral-derivatives-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0',\n",
    "    'msd-mvd-v1.0',\n",
    "    'msd-rh-v1.0',\n",
    "    'msd-rp-v1.0',\n",
    "    'msd-ssd-v1.0',\n",
    "    'msd-trh-v1.0',\n",
    "    'msd-tssd-v1.0',\n",
    "]\n",
    "\n",
    "lookup = {\n",
    "    'real': DoubleType(),\n",
    "    'NUMERIC': DoubleType(),\n",
    "    'float': DoubleType(),\n",
    "    'string': StringType(),\n",
    "    'STRING': StringType(),\n",
    "}\n",
    "# Choose a dataset name, load attribute names, and define schemas based on attribute names\n",
    "\n",
    "name = 'msd-jmir-area-of-moments-all-v1.0'\n",
    "\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "])\n",
    "metadata = spark.read.csv(f'/data/msd/audio/attributes/{name}.attributes.csv', schema=metadata_schema)\n",
    "\n",
    "metadata.show(metadata.count(), truncate=False)\n",
    "schema_actual = StructType([\n",
    "    StructField(name, lookup[typename], True) for name, typename in metadata.collect()\n",
    "])\n",
    "\n",
    "schema_simple = StructType([\n",
    "    StructField(f\"F{i:03d}\", DoubleType(), True) for i in range(0, metadata.count() - 1)\n",
    "] + [\n",
    "    StructField(f\"ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "print('name = ' + name)\n",
    "print('')\n",
    "print('actual_schema = ' + pretty(schema_actual))\n",
    "print('')\n",
    "print('simple_schema = ' + pretty(schema_simple))\n",
    "print('')\n",
    "\n",
    "data_actual = spark.read.csv(f'/data/msd/audio/features/{name}.csv', schema=schema_actual, quote=\"'\")\n",
    "data_simple = spark.read.csv(f'/data/msd/audio/features/{name}.csv', schema=schema_simple, quote=\"'\")\n",
    "\n",
    "show_as_html(data_actual)\n",
    "show_as_html(data_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Analyse audio feature datasets. Pick one of dataset to analyse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Produce descriptive statistics for audio features? Are any features strongly correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset names and attribute type mapping\n",
    "\n",
    "datasets = [\n",
    "    'msd-jmir-area-of-moments-all-v1.0',\n",
    "    'msd-jmir-lpc-all-v1.0',\n",
    "    'msd-jmir-methods-of-moments-all-v1.0',\n",
    "    'msd-jmir-mfcc-all-v1.0',\n",
    "    'msd-jmir-spectral-all-all-v1.0',\n",
    "    'msd-jmir-spectral-derivatives-all-all-v1.0',\n",
    "    'msd-marsyas-timbral-v1.0',\n",
    "    'msd-mvd-v1.0',\n",
    "    'msd-rh-v1.0',\n",
    "    'msd-rp-v1.0',\n",
    "    'msd-ssd-v1.0',\n",
    "    'msd-trh-v1.0',\n",
    "    'msd-tssd-v1.0',\n",
    "]\n",
    "\n",
    "lookup = {\n",
    "    'real': DoubleType(),\n",
    "    'NUMERIC': DoubleType(),\n",
    "    'float': DoubleType(),\n",
    "    'string': StringType(),\n",
    "    'STRING': StringType(),\n",
    "}\n",
    "# Choose a dataset name, load attribute names, and define schemas based on attribute names\n",
    "\n",
    "name =  'msd-jmir-mfcc-all-v1.0'\n",
    "\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "])\n",
    "metadata = spark.read.csv(f'/data/msd/audio/attributes/{name}.attributes.csv', schema=metadata_schema)\n",
    "\n",
    "# metadata.show(metadata.count(), truncate=False)\n",
    "schema_actual = StructType([\n",
    "    StructField(name, lookup[typename], True) for name, typename in metadata.collect()\n",
    "])\n",
    "\n",
    "schema_simple = StructType([\n",
    "    StructField(f\"F{i:03d}\", DoubleType(), True) for i in range(0, metadata.count() - 1)\n",
    "] + [\n",
    "    StructField(f\"ID\", StringType(), True)\n",
    "])\n",
    "\n",
    "print('name = ' + name)\n",
    "print('')\n",
    "print('actual_schema = ' + pretty(schema_actual))\n",
    "print('')\n",
    "print('simple_schema = ' + pretty(schema_simple))\n",
    "print('')\n",
    "\n",
    "data_actual = spark.read.csv(f'/data/msd/audio/features/{name}.csv', schema=schema_actual, quote=\"'\")\n",
    "data_simple = spark.read.csv(f'/data/msd/audio/features/{name}.csv', schema=schema_simple, quote=\"'\")\n",
    "\n",
    "show_as_html(data_actual)\n",
    "show_as_html(data_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_simple\n",
    "# features.printSchema()\n",
    "print(features.count())  \n",
    "# 994623\n",
    "#drop non-numeric column\n",
    "desc_stats=features.drop(\"MSD_TRACKID\")\n",
    "\n",
    "#find Descriptive statistics of each column feature\n",
    "combined_stats = desc_stats.describe().toPandas().transpose()\n",
    "combined_stats.columns = ['Count', 'Mean', 'Stddev', 'Min', 'Max']\n",
    "combined_stats['Feature'] = combined_stats.index\n",
    "combined_stats = combined_stats[['Feature', 'Count', 'Mean', 'Stddev', 'Min', 'Max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlated features  \n",
    "corr_data = features.toPandas()\n",
    "corr_data.corr().unstack().sort_values().drop_duplicates().tail(11) #Top 10 correlated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "correlation_matrix = corr_data.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(18, 16))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1, mask=mask)  # Set vmin and vmax to -1 and 1 respectively\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Load the MSD All Music Genre Dataset (MAGD). Visualize the distribution of genres for the songs that were matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs:///data/msd/genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -cat /data/msd/genre/msd-topMAGD-genreAssignment.tsv | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_schema = StructType([\n",
    "    StructField(\"track_id\", StringType(), True),\n",
    "    StructField(\"genre\", StringType(), True)\n",
    "])\n",
    "genres_data = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"false\")\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .schema(genre_schema)\n",
    "    .load(\"hdfs:///data/msd/genre/msd-MAGD-genreAssignment.tsv\")\n",
    "    .cache()\n",
    ")\n",
    "genres_data.show(5, False)\n",
    " \n",
    "\n",
    "# First, remove the songs that were mismatched from genre_schema based on \"track_id\"\n",
    "genre_matched = genres_data.join(mismatches_not_accepted, on=\"track_id\", how=\"left_anti\")  \n",
    "\n",
    "# then plot distribution using matplotlib\n",
    "\n",
    "genre_plot_data = genre_matched.groupBy('genre').count() \n",
    "\n",
    "genre_plot_data = genre_plot_data.sort(\"count\",ascending=False) #sort by count\n",
    "genre_plot_data.show(21)\n",
    "\n",
    "genre_plot_data = genre_plot_data.toPandas() #Pandas format to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_genre(data):\n",
    "    plt.figure(figsize=(10, 10))  # Increase the width of the plot by adjusting figsize\n",
    "    ax = data.plot(kind='bar', x='genre', y='count', legend=False, grid=True)\n",
    "\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Genre')\n",
    "    plt.title(\"Total Number Of Matched Songs Per Genre\")\n",
    "\n",
    "    # Adjust gridlines width\n",
    "    ax.yaxis.grid(linewidth=0.5)\n",
    "    ax.xaxis.grid(linewidth=0.5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_genre(genre_plot_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Merge the genres dataset and the audio features dataset so that every song has a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls hdfs:///data/msd/genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=features.withColumnRenamed('MSD_TRACKID', 'ID')\n",
    "show_as_html(features, 5)\n",
    "show_as_html(genres_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_features_genre=genres_data.join(features,genres_data.track_id==features.ID,how='inner') #inner join\n",
    "mfcc_features_genre.printSchema()\n",
    "mfcc_features_genre.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Develop a binary classification model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Convert the genre column into a column representing if the song is ”Electronic” or some other genre as a binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_genre_binary = mfcc_features_genre.withColumn(\"Class\", F.when((F.col(\"genre\").isin({\"Electronic\"})), 1).otherwise(0)) ## labeliing binary class\n",
    "show_as_html(mfcc_genre_binary,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the class balance of the binary label?\n",
    "show_as_html(mfcc_genre_binary.groupBy(\"Class\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Split the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping out non-numeric columns \n",
    "mfcc_genre_df = mfcc_genre_binary.drop(\"ID\",\"track_id\",\"genre\")\n",
    "mfcc_genre_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove high-correlated variables. we delete the variable F011.\n",
    "mfcc_genre_df = mfcc_genre_df.drop(\"F011\", \"F008\")\n",
    "corr_data = mfcc_genre_df.toPandas() \n",
    "corr_data.corr().unstack().sort_values().drop_duplicates().tail(11) #Top 10 correlated pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "assembler = (VectorAssembler()\n",
    "            .setInputCols(mfcc_genre_df.columns[:-1])\n",
    "            .setOutputCol('features')\n",
    "            )\n",
    "features = assembler.transform(mfcc_genre_df).select([\"features\", \"Class\"])\n",
    "features.count()  \n",
    "# 420620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_as_html(features,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test and train set use exact stratification using Window\n",
    "temp = (\n",
    "     features\n",
    "     .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "     .withColumn(\"Random\",F.rand())\n",
    "     .withColumn(\n",
    "        \"Row\",\n",
    "         F.row_number()\n",
    "         .over(\n",
    "             Window\n",
    "             .partitionBy(\"Class\")\n",
    "             .orderBy(\"Random\")\n",
    "        )\n",
    "     )\n",
    " )\n",
    "show_as_html(temp,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = temp.where(\n",
    "     ((F.col(\"Class\") == 0) & (F.col(\"Row\") < 379954 * 0.8)) |\n",
    "     ((F.col(\"Class\") == 1) & (F.col(\"Row\") < 40666 * 0.8))     ## 80 % stratified split on class label column.\n",
    ")\n",
    "training.cache()\n",
    "\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.filter(F.col(\"Class\") == 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.drop(\"id\", \"Random\", \"Row\")  ## Training data\n",
    "test = test.drop(\"id\", \"Random\", \"Row\")          ## Testing data (only used for final predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined function that shows the class balance for a given dataframe\n",
    "def print_class_balance(data, name):\n",
    "    N = data.count()\n",
    "    counts = data.groupBy(\"Class\").count().toPandas()\n",
    "    counts[\"ratio\"] = counts[\"count\"] / N\n",
    "    print(name)\n",
    "    print(N)\n",
    "    print(counts)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_class_balance(features, \"features\")\n",
    "print_class_balance(training, \"training\")\n",
    "print_class_balance(test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling\n",
    "training_downsampled = (\n",
    "    training\n",
    "    .withColumn(\"Random\", F.rand())\n",
    "    .where((F.col(\"Class\") != 0) | ((F.col(\"Class\") == 0) & (F.col(\"Random\") < 2 * (40666 / 379954))))\n",
    ")\n",
    "training_downsampled.cache()\n",
    "\n",
    "print_class_balance(training_downsampled, \"training_downsampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d-e) train & assess the performance of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# User define for performance metrics\n",
    "def binary_metrics(df):\n",
    "    \"\"\"\n",
    "    Calculate and Print the classification Metrics\n",
    "    \"\"\"\n",
    "    total = df.count()\n",
    "    TP = df[(df.Class == 1) & (df.prediction == 1)].count()\n",
    "    TN = df[(df.Class == 0) & (df.prediction == 0)].count()\n",
    "    FP = df[(df.Class == 0) & (df.prediction == 1)].count()\n",
    "    FN = df[(df.Class== 1) & (df.prediction == 0)].count()\n",
    "         \n",
    "    a = (TP + TN) / total #accuracy\n",
    "         \n",
    "    #check 0 value for recall\n",
    "    if (TP + FN) != 0:\n",
    "        r = float(TP)/(TP + FN)\n",
    "    else:\n",
    "        r = \"NA\"\n",
    "         \n",
    "    #check 0 value for precision\n",
    "    if (TP + FP) != 0:\n",
    "        p = float(TP)/(TP + FP)\n",
    "    else:\n",
    "        p = \"NA\"\n",
    "            \n",
    "    #check 0 value for f1 score\n",
    "    if p == \"NA\" or r == \"NA\":\n",
    "        f = \"NA\"\n",
    "    else:\n",
    "        f = 1/((1/r+1/p)/2)\n",
    "            \n",
    "    binary_class_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"Class\", metricName=\"areaUnderROC\")\n",
    "    auroc = binary_class_evaluator.evaluate(df)\n",
    "    print('Actual count: {}'.format(total))\n",
    "    print('True Positive: {}'.format(TP))\n",
    "    print('True Negative: {}'.format(TN))\n",
    "    print('False Positive: {}'.format(FP))\n",
    "    print('False Negative: {}'.format(FN))\n",
    "    print('precision: {}'.format(p))\n",
    "    print('recall: {}'.format(r))\n",
    "    print('accuracy: {}'.format(a))\n",
    "    print('auroc: {}'.format(auroc))\n",
    "    print('F1_Score: {}'.format(f))\n",
    "    return()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# No sampling\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Class')\n",
    "lr_model1 = lr.fit(training)\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions1 = lr_model1.transform(test)\n",
    "predictions1.cache()\n",
    "binary_metrics(predictions1)\n",
    "\n",
    "# Downsampling\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Class')\n",
    "lr_model2 = lr.fit(training_downsampled)\n",
    "predictions2 = lr_model2.transform(test)\n",
    "predictions2.cache()\n",
    "binary_metrics(predictions2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOM FOREST\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create an initial RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\")\n",
    "\n",
    "# Nosampling\n",
    "rfModel1 = rf.fit(training)\n",
    "predictions_rf1 = rfModel1.transform(test)\n",
    "binary_metrics(predictions_rf1)\n",
    "\n",
    "# Downsampling\n",
    "rfModel2 = rf.fit(training_downsampled)\n",
    "predictions_rf2 = rfModel2.transform(test)\n",
    "binary_metrics(predictions_rf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "lsvc = LinearSVC(labelCol=\"Class\", featuresCol=\"features\")\n",
    "\n",
    "# Nosampling\n",
    "LinearSVC_model1 = lsvc.fit(training)\n",
    "predictions_svc1 = LinearSVC_model1.transform(test)\n",
    "binary_metrics(predictions_svc1)\n",
    "\n",
    "# Downsampling\n",
    "LinearSVC_model2 = lsvc.fit(training_downsampled)\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "predictions_svc2 = LinearSVC_model2.transform(test)\n",
    "binary_metrics(predictions_svc2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Tunning\n",
    "# Logistic Regression\n",
    "\n",
    "# declaring estimator for CV\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "lr = LogisticRegression(labelCol=\"Class\", featuresCol=\"features\")\n",
    "\n",
    "# building grid\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "paramGrid = ParamGridBuilder()\\\n",
    ".addGrid(lr.elasticNetParam,[0.0, 0.1,0.3, 0.5, 0.8, 1.0])\\\n",
    ".addGrid(lr.maxIter,[10, 100])\\\n",
    ".addGrid(lr.regParam,[0,0.01, 0.5, 1.0]) \\\n",
    ".build()\n",
    "\n",
    "# building evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",labelCol=\"Class\")\n",
    "#evaluator.evaluate(predictions)\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations and print result\n",
    "cvModel = cv.fit(training_downsampled)\n",
    "bestmodel=cvModel.bestModel\n",
    "predictions_lr_cv = bestmodel.transform(test)\n",
    "binary_metrics(predictions_lr_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass Classification\n",
    "# Logistic Regression\n",
    "#check schema again\n",
    "mfcc_features_genre.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelling genres using stringindexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer().setInputCol(\"genre\").setOutputCol(\"label\")\n",
    "indexed_df = indexer.fit(mfcc_features_genre).transform(mfcc_features_genre)\n",
    "indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_df.groupBy(\"label\",\"genre\").count().orderBy(\"label\").show(21,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns which we don't need\n",
    "multi_genre = indexed_df.drop(\"ID\",\"genre\",\"track_id\")\n",
    "# drop columns which are highly correlated\n",
    "multi_genre = multi_genre.drop(\"F008\",\"F011\")\n",
    "multi_genre.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_balance(data, name):\n",
    "    N = data.count()\n",
    "    counts = data.groupBy(\"label\").count().orderBy(\"label\").toPandas()\n",
    "    counts[\"ratio\"] = counts[\"count\"] / N\n",
    "    print(name)\n",
    "    print(N)\n",
    "    print(counts)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble features\n",
    "assembler = (VectorAssembler()\n",
    "    .setInputCols(multi_genre.columns[:-1])\n",
    "    .setOutputCol(\"features\"))\n",
    "multi_features = assembler.transform(multi_genre).select([\"features\", \"label\"])\n",
    "\n",
    "# splitting data using exact stratification\n",
    "temp = (\n",
    "    multi_features\n",
    "    .withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    .withColumn(\"Random\", F.rand())\n",
    "    .withColumn(\n",
    "        \"Row\",\n",
    "        F.row_number()\n",
    "        .over(\n",
    "            Window\n",
    "            .partitionBy(\"label\")\n",
    "            .orderBy(\"Random\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "class_counts = (\n",
    "      multi_features\n",
    "     .groupBy(\"label\")\n",
    "     .count()\n",
    "     .toPandas()\n",
    "     .set_index(\"label\")[\"count\"]\n",
    "     .to_dict()\n",
    " )\n",
    "classes = sorted(class_counts.keys())\n",
    "\n",
    "training = temp\n",
    " \n",
    "for c in classes:\n",
    "     training = training.where((F.col(\"label\") != c) | (F.col(\"Row\") < class_counts[c] * 0.8))\n",
    "\n",
    "training.cache()\n",
    "\n",
    "test = temp.join(training, on=\"id\", how=\"left_anti\")\n",
    "test.cache()\n",
    "\n",
    "training = training.drop(\"id\", \"Random\", \"Row\")\n",
    "test = test.drop(\"id\", \"Random\", \"Row\")\n",
    "\n",
    "print_class_balance(multi_features,\"multi_features\")\n",
    "print_class_balance(training, \"training\")\n",
    "print_class_balance(test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsampling applied for the major genres which are \"Pop_rock\" and \"electronic\"\n",
    "downsample=training.filter(training[\"label\"].isin([0,1])) \n",
    "training_downsampled = downsample.sampleBy(\"label\",fractions = {0:0.10,1:0.50}, seed = 688)\n",
    "training_downsampled.cache()\n",
    "\n",
    "# The final train data\n",
    "rest = training.filter(training[\"label\"].isin([2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]))\n",
    "afterdown_train_data = training_downsampled.union(rest)\n",
    "print_class_balance(afterdown_train_data, \"afterdown_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling via poisson random sampling\n",
    "\n",
    "counts = {label: count for label, count in training.groupBy(\"label\").count().collect()}\n",
    "count_lower_bound = 2000\n",
    "count_upper_bound = 500000\n",
    "\n",
    "def random_resample(x, counts, count_lower_bound, count_upper_bound):\n",
    "\n",
    "    count = counts[x]\n",
    "\n",
    "    if count < count_lower_bound:\n",
    "        return [x] * int(1 + np.random.poisson((count_lower_bound - count) / count))  # randomly upsample to count_lower_bound\n",
    "\n",
    "    if count > count_upper_bound:\n",
    "        if np.random.rand() < count_upper_bound / count: # randomly downsample to count_upper_bound\n",
    "            return [x]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    return [x]  # do nothing\n",
    "\n",
    "random_resample_udf = F.udf(lambda x: random_resample(x, counts, count_lower_bound, count_upper_bound), ArrayType(IntegerType()))\n",
    "final_train_data  = (\n",
    "    afterdown_train_data\n",
    "    .withColumn(\"sample\", random_resample_udf(F.col(\"label\")))\n",
    "    .select(\n",
    "        F.col(\"label\"),\n",
    "        F.col(\"features\"),\n",
    "        F.explode(F.col(\"sample\")).alias(\"sample\")\n",
    "    )\n",
    "    .drop(\"sample\")\n",
    ")\n",
    "\n",
    "print_class_balance(final_train_data , \"final_train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UP AND DOWN SAMPLING TRAINING\n",
    "# Multi class Classification using Logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# instantiate the base classifier.\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# instantiate the One Vs Rest Classifier.\n",
    "ovr = OneVsRest(classifier=lr)\n",
    "\n",
    "# train the multiclass model.\n",
    "ovrModel = ovr.fit(final_train_data)\n",
    "\n",
    "# score the model on test data.\n",
    "predictions = ovrModel.transform(test)\n",
    "\n",
    "multiclass_evaluator(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Song recommendations\n",
    "## Q1: Taste Profile exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /data/msd/tasteprofile/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) How many unique songs are there in the dataset?\n",
    "unique_songs = triplets_not_mismatched.select(\"song_id\").distinct().count()\n",
    "print('number of unique songs is: ', unique_songs) #378310\n",
    "# (a) How many unique users?\n",
    "unique_users = triplets_not_mismatched.select(\"user_id\").distinct().count()\n",
    "print('number of unique users is: ', unique_users) #1019318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets_not_mismatched.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (b) MOST ACTIVE USER ANALYSIS\n",
    "user_counts = triplets_not_mismatched.groupBy(\"user_id\").agg(\n",
    "                                      F.count(F.col(\"song_id\")).alias(\"song_count\"),\n",
    "                                      F.sum(F.col(\"plays\")).alias(\"play_count\")\n",
    "                                     ).orderBy(F.col(\"play_count\").desc()) # Check most active user by counting play times\n",
    "  \n",
    "user_counts.cache()\n",
    "\n",
    "print('There are ', user_counts.count(), ' unique users') #1,019,318\n",
    "print('Top 5 most active users who have highest play count:')\n",
    "user_counts.show(5, False)\n",
    "# What is this as a percentage of the total number of unique songs in the dataset?\n",
    "song_most_user = user_counts.toPandas()['song_count'][0] # song count the most active user played\n",
    "print('Percentage of songs the most active user played: ',100 * song_most_user / unique_songs) #0.052%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_statistics = (\n",
    "    user_counts.select(\"song_count\", \"play_count\")\n",
    "    .describe()\n",
    "    .toPandas()\n",
    "    .set_index(\"summary\")\n",
    "    .rename_axis(None)\n",
    ")\n",
    "print(user_statistics)\n",
    "\n",
    "print(\"---User with the number of unique song quantile:---\")\n",
    "print(user_counts.approxQuantile(\"song_count\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.05))  #FOR SONG RECOMMENDATION\n",
    "print(\"---User with number of total activity quantile:---\")\n",
    "print(user_counts.approxQuantile(\"play_count\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Song statistics\n",
    "song_counts = triplets_not_mismatched.groupBy(\"song_id\").agg(\n",
    "      F.count(F.col(\"user_id\")).alias(\"user_count\"),\n",
    "      F.sum(F.col(\"plays\")).alias(\"play_count\"),\n",
    "    ).orderBy(F.col(\"play_count\").desc())\n",
    "song_counts.cache()\n",
    "print('Number of unique song: ', song_counts.count()) #378310\n",
    "print('Top 5 most active users who have highest play count:')\n",
    "song_counts.show(5, False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOST POPULAR SONG ANALYSIS\n",
    "statistics = (\n",
    "  song_counts\n",
    "  .select(\"user_count\", \"play_count\")\n",
    "  .describe()\n",
    "  .toPandas()\n",
    "  .set_index(\"summary\")\n",
    "  .rename_axis(None)\n",
    ")\n",
    "print(statistics)\n",
    "\n",
    "# Aprroxiamte Quantitle\n",
    "print(\"---songs with have most users listen to \")\n",
    "print(song_counts.approxQuantile(\"user_count\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.05))\n",
    "print(\"---songs with most plays (most popularity)---\") \n",
    "print(song_counts.approxQuantile(\"play_count\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.05)) #FOR SONG RECOMMENDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(data):\n",
    "    \"\"\"histogram showing the distribution of data\"\"\"\n",
    "    if data.columns[0] == \"song_id\":\n",
    "        name = \"Song\"\n",
    "        title = \"Song Popularity\"\n",
    "    elif data.columns[0] == \"user_id\":\n",
    "        name = \"User\"\n",
    "        title = \"User Activity\"\n",
    "    fig, ax = plt.subplots(figsize=(16, 4))  # Increase the width of the plot to double\n",
    "    data.hist(column='play_count', bins=500, ax=ax)\n",
    "    ax.set_xlabel(f'Number of plays Range per {name}')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {title}')\n",
    "    current_values_y = ax.get_yticks()\n",
    "    current_values_x = ax.get_xticks()\n",
    "    ax.set_xticklabels(['{:,.0f}'.format(x) for x in current_values_x])\n",
    "    ax.set_yticklabels(['{:,.0f}'.format(x) for x in current_values_y])\n",
    "    ax.grid(False)  # Remove the grid lines\n",
    "    plt.savefig(f'Distribution of {title}', bbox_inches='tight', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(songs_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(user_activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print distribution of 'Song play count' (N) and 'User song count' (M):\n",
    "print('User song count M: ', user_counts.approxQuantile(\"song_count\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.05))\n",
    "print('Song play count N: ', song_counts.approxQuantile(\"play_count\", [0.0, 0.25, 0.5, 0.75, 1.0], 0.05))\n",
    "# (d) set threshold M & N. We will repeat these steps with other 2 pairs M & N\n",
    "triplets_not_mismatched.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_counts.show(5)\n",
    "user_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  M = 13, N = 5 (remove data before first quantile)\n",
    "user_song_count_threshold = 13  #M\n",
    "song_play_count_threshold = 5 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "    \n",
    "# (e) Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "show_as_html(triplets_limited,5)\n",
    "\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:    {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "\n",
    "test_not_training.show(50, False)\n",
    "\n",
    "\n",
    "# add test_not_training to dictionary\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "counts\n",
    "\n",
    "# remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 25% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_counts(triplets):\n",
    "  return (\n",
    "    triplets\n",
    "    .groupBy(\"user_id_encoded\")\n",
    "    .agg(\n",
    "      F.count(F.col(\"song_id_encoded\")).alias(\"song_count\"),\n",
    "      F.sum(F.col(\"plays\")).alias(\"play_count\"),\n",
    "    )\n",
    "    .orderBy(F.col(\"play_count\").desc())\n",
    "  )\n",
    "\n",
    "user_counts_1 = get_user_counts(test)\n",
    "user_counts_1.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_users = user_counts_1.select(\"user_id_encoded\").limit(20).rdd.flatMap(lambda x: x).collect()\n",
    "# we got top 20 users:\n",
    "print(top_20_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import require librabry\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "# Randomly pick few users for recommendation\n",
    "subset_users = test.filter(F.col(\"user_id_encoded\").isin([24067,9,515326,208506,36877,18092,678615,47605,8,659627,76,5168]))\n",
    "some_predictions = als_model.transform(subset_users)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_users_recommend = als_model.recommendForUserSubset(subset_users, 10)\n",
    "subset_users_recommend.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_songs(x):\n",
    "    x = sorted(x, key=lambda x: -x[1])\n",
    "    return [x[0] for x in x]\n",
    "extract_songs_udf = F.udf(lambda x: extract_songs(x), ArrayType(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the subset song recommended and subset actual song for each user:\n",
    "\n",
    "# Song recommendation\n",
    "\n",
    "subset_users_song_recommend = (\n",
    "    subset_users_recommend.withColumn('songs_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'songs_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "\n",
    "subset_users_song_recommend.show(40, False)\n",
    "\n",
    "# Actual song Users play:\n",
    "\n",
    "subset_actual_songs = (\n",
    "    subset_users\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"subset_actual_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"subset_actual_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "\n",
    "subset_actual_songs.cache()\n",
    "subset_actual_songs.show(10, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   EXPERIMENT WITH DIFFERENT M AND N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 1\n",
    "# M:13, N:5\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2\n",
    "#  M = 13, N = 8\n",
    "user_song_count_threshold = 13  #M\n",
    "song_play_count_threshold = 8 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 3\n",
    "#  M = 15, N = 8\n",
    "user_song_count_threshold = 15  #M\n",
    "song_play_count_threshold = 8 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 4\n",
    "#  M = 13, N = 10\n",
    "user_song_count_threshold = 13  #M\n",
    "song_play_count_threshold = 10 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 5\n",
    "#  M = 11, N = 5\n",
    "user_song_count_threshold = 11  #M\n",
    "song_play_count_threshold = 5 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6\n",
    "#  M = 9, N = 5\n",
    "user_song_count_threshold = 9  #M\n",
    "song_play_count_threshold = 5 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 6\n",
    "#  M = 8, N = 5\n",
    "user_song_count_threshold = 8  #M\n",
    "song_play_count_threshold = 5 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 8\n",
    "#  M =7, N = 5\n",
    "user_song_count_threshold = 7  #M\n",
    "song_play_count_threshold = 5 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 9\n",
    "#  M =6, N = 5\n",
    "user_song_count_threshold = 6  #M\n",
    "song_play_count_threshold = 5 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 10\n",
    "#  M =7, N = 4\n",
    "user_song_count_threshold = 7  #M\n",
    "song_play_count_threshold = 4 #N\n",
    "triplets_limited = triplets_not_mismatched\n",
    "# Filter triplets_limited by threshold M & N\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    user_counts.where(F.col(\"song_count\") > user_song_count_threshold).select(\"user_id\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "\n",
    "triplets_limited = (\n",
    "  triplets_limited\n",
    "  .join(\n",
    "    song_counts.where(F.col(\"play_count\") > song_play_count_threshold).select(\"song_id\"),\n",
    "    on=\"song_id\",\n",
    "    how=\"inner\"\n",
    "  )\n",
    ")\n",
    "triplets_limited.cache()\n",
    "triplets_limited_count = triplets_limited.count()\n",
    "print('Number of remained observation: ', triplets_limited_count) #43,379,863/45,795,111 -->5.27% data was removed\n",
    "print((1 - triplets_limited_count/45795111)*100, ' % observations were removed')\n",
    "print('')\n",
    "print('------------------------')\n",
    "print('')\n",
    "# ------------------------------\n",
    "# Encoding\n",
    "\n",
    "user_id_indexer = StringIndexer(inputCol=\"user_id\", outputCol=\"user_id_encoded\")\n",
    "song_id_indexer = StringIndexer(inputCol=\"song_id\", outputCol=\"song_id_encoded\")\n",
    "\n",
    "user_id_indexer_model = user_id_indexer.fit(triplets_limited)\n",
    "song_id_indexer_model = song_id_indexer.fit(triplets_limited)\n",
    "\n",
    "triplets_limited = user_id_indexer_model.transform(triplets_limited)\n",
    "triplets_limited = song_id_indexer_model.transform(triplets_limited)\n",
    "# ------------------------------\n",
    "# Splitting\n",
    "\n",
    "training, test = triplets_limited.randomSplit([0.7, 0.3])\n",
    "\n",
    "test_not_training = test.join(training, on=\"user_id\", how=\"left_anti\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()\n",
    "test_not_training.cache()\n",
    "\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "print(f\"test_not_training: {test_not_training.count()}\")\n",
    "print('')\n",
    "test_not_training.show(50, False)\n",
    "print('')\n",
    "\n",
    "counts = test_not_training.groupBy(\"user_id\").count().toPandas().set_index(\"user_id\")[\"count\"].to_dict()\n",
    "\n",
    "print(counts)\n",
    "\n",
    "#remove the test not in training from test set\n",
    "for k, v in counts.items():\n",
    "  test = test.where((F.col(\"user_id\") != k))\n",
    "\n",
    "#Print final training test\n",
    "print(f\"training:      {training.count()}\")\n",
    "print(f\"test:        {test.count()}\")\n",
    "\n",
    "# checking test set if it contains at least 20% of the plays in total\n",
    "test_play = test.agg(F.sum(F.col(\"plays\"))).collect() # test play count\n",
    "total_play = triplets_limited.agg(F.sum(F.col(\"plays\"))).collect() #total play count\n",
    "print(100 * test_play[0][0] / total_play[0][0]) # test play count percentage\n",
    "\n",
    "#ALS model training\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"user_id_encoded\", itemCol=\"song_id_encoded\", ratingCol=\"plays\", implicitPrefs=True)\n",
    "als_model = als.fit(training)\n",
    "\n",
    "predictions = als_model.transform(test)\n",
    "recommended_songs = als_model.recommendForAllUsers(10)\n",
    "\n",
    "user_song_recommendation = (\n",
    "    recommended_songs.withColumn('user_song_recommendation', extract_songs_udf(F.col('recommendations')))\n",
    "    .select(['user_id_encoded', 'user_song_recommendation']).orderBy(F.col(\"user_id_encoded\"))\n",
    "    )\n",
    "print('')\n",
    "user_song_recommendation.cache()\n",
    "user_song_recommendation.show(5,70)\n",
    "\n",
    "relevant_songs = (\n",
    "    test\n",
    "    .select(\n",
    "        F.col(\"user_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"song_id_encoded\").cast(IntegerType()),\n",
    "        F.col(\"plays\").cast(IntegerType())\n",
    "    )\n",
    "    .groupBy('user_id_encoded')\n",
    "    .agg(\n",
    "        F.collect_list(\n",
    "            F.array(\n",
    "                F.col(\"song_id_encoded\"),\n",
    "                F.col(\"plays\")\n",
    "            )\n",
    "        ).alias('relevance')\n",
    "    )\n",
    "    .withColumn(\"actual_listened_songs\", extract_songs_udf(F.col(\"relevance\")))\n",
    "    .select(\"user_id_encoded\", \"actual_listened_songs\")\n",
    "    .orderBy(F.col(\"user_id_encoded\"))\n",
    ")\n",
    "print('')\n",
    "relevant_songs.cache()\n",
    "relevant_songs.show(5,70)\n",
    "print('')\n",
    "perUserItemsRDD = (\n",
    "    user_song_recommendation.join(relevant_songs, on='user_id_encoded', how='inner')\n",
    "    .rdd\n",
    "    .map(lambda row: (row[1], row[2]))\n",
    ")\n",
    "perUserItemsRDD.cache()\n",
    "\n",
    "# generating metrics required\n",
    "rankingMetrics = RankingMetrics(perUserItemsRDD)\n",
    "print(f'Precision @10: {rankingMetrics.precisionAt(10)}') # Precision at 10\n",
    "print(f'NDCG @10: {rankingMetrics.ndcgAt(10)}') #NDCG at 10\n",
    "print(f'Mean Average Precision (MAP): {rankingMetrics.meanAveragePrecision}') #MAP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
